{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "c4012e1d-26a9-48c0-a16b-a30f9d46ae5c",
      "cell_type": "code",
      "source": "import numpy as np\n\ndef sigmoid(z):\n    return 1/(1 + np.exp(-z))\n\ndef initialize_parameters(n_x, n_h, n_y):\n    W1 = np.random.randn(n_h, n_x)\n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y, n_h)\n    b2 = np.zeros((n_y, 1))\n\n    parameters = {\n        \"W1\": W1,\n        \"b1\" : b1,\n        \"W2\": W2,\n        \"b2\" : b2\n    }\n    return parameters\n\ndef forward_prop(X, parameters):\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n\n    Z1 = np.dot(W1, X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = sigmoid(Z2)\n\n    cache = {\n        \"A1\": A1,\n        \"A2\": A2\n    }\n    return A2, cache\n\ndef calculate_cost(A2, Y):\n    cost = -np.sum(np.multiply(Y, np.log(A2)) +  np.multiply(1-Y, np.log(1-A2)))/m\n    cost = np.squeeze(cost)\n\n    return cost\n\ndef backward_prop(X, Y, cache, parameters):\n    A1 = cache[\"A1\"]\n    A2 = cache[\"A2\"]\n\n    W2 = parameters[\"W2\"]\n\n    dZ2 = A2 - Y\n    dW2 = np.dot(dZ2, A1.T)/m\n    db2 = np.sum(dZ2, axis=1, keepdims=True)/m\n    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1-np.power(A1, 2))\n    dW1 = np.dot(dZ1, X.T)/m\n    db1 = np.sum(dZ1, axis=1, keepdims=True)/m\n\n    grads = {\n        \"dW1\": dW1,\n        \"db1\": db1,\n        \"dW2\": dW2,\n        \"db2\": db2\n    }\n\n    return grads\n\ndef update_parameters(parameters, grads, learning_rate):\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n\n    dW1 = grads[\"dW1\"]\n    db1 = grads[\"db1\"]\n    dW2 = grads[\"dW2\"]\n    db2 = grads[\"db2\"]\n\n    W1 = W1 - learning_rate*dW1\n    b1 = b1 - learning_rate*db1\n    W2 = W2 - learning_rate*dW2\n    b2 = b2 - learning_rate*db2\n    \n    new_parameters = {\n        \"W1\": W1,\n        \"W2\": W2,\n        \"b1\" : b1,\n        \"b2\" : b2\n    }\n\n    return new_parameters\n\n\ndef model(X, Y, n_x, n_h, n_y, num_of_iters, learning_rate):\n    parameters = initialize_parameters(n_x, n_h, n_y)\n\n    for i in range(0, num_of_iters+1):\n        a2, cache = forward_prop(X, parameters)\n\n        cost = calculate_cost(a2, Y)\n\n        grads = backward_prop(X, Y, cache, parameters)\n\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n        if(i%100 == 0):\n            print('Cost after iteration# {:d}: {:f}'.format(i, cost))\n\n    return parameters\n\ndef predict(X, parameters):\n    a2, cache = forward_prop(X, parameters)\n    yhat = a2\n    yhat = np.squeeze(yhat)\n    if(yhat >= 0.5):\n        y_predict = 1\n    else:\n        y_predict = 0\n\n    return y_predict\n    \n\n\nnp.random.seed(2)\n\n# The 4 training examples by columns\nX = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n\n# The outputs of the XOR for every example in X\nY = np.array([[0, 1, 1, 0]])\n\n# No. of training examples\nm = X.shape[1]\n\n# Set the hyperparameters\nn_x = 2     #No. of neurons in first layer\nn_h = 2     #No. of neurons in hidden layer\nn_y = 1     #No. of neurons in output layer\nnum_of_iters = 1000\nlearning_rate = 0.3\n\ntrained_parameters = model(X, Y, n_x, n_h, n_y, num_of_iters, learning_rate)\n\n# Test 2X1 vector to calculate the XOR of its elements. \n# Try (0, 0), (0, 1), (1, 0), (1, 1)\nX_test = np.array([[1], [1]])\n\ny_predict = predict(X_test, trained_parameters)\n\nprint('Neural Network prediction for example ({:d}, {:d}) is {:d}'.format(\n    X_test[0][0], X_test[1][0], y_predict))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Cost after iteration# 0: 0.856267\nCost after iteration# 100: 0.347426\nCost after iteration# 200: 0.101195\nCost after iteration# 300: 0.053631\nCost after iteration# 400: 0.036031\nCost after iteration# 500: 0.027002\nCost after iteration# 600: 0.021543\nCost after iteration# 700: 0.017896\nCost after iteration# 800: 0.015293\nCost after iteration# 900: 0.013344\nCost after iteration# 1000: 0.011831\nNeural Network prediction for example (1, 1) is 0\n"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "b5e05f71-960a-44b3-80d9-e1592b33e21b",
      "cell_type": "code",
      "source": "import numpy as np\n\ndef sigmoid(z):\n    return 1/(1 + np.exp(-z))\n\ndef initialize_parameters(n_x, n_h, n_y):\n    W1 = np.random.randn(n_h, n_x)\n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y, n_h)\n    b2 = np.zeros((n_y, 1))\n\n    parameters = {\n        \"W1\": W1,\n        \"b1\" : b1,\n        \"W2\": W2,\n        \"b2\" : b2\n    }\n    return parameters\n\ndef forward_prop(X, parameters):\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n\n    Z1 = np.dot(W1, X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = sigmoid(Z2)\n\n    cache = {\n        \"A1\": A1,\n        \"A2\": A2\n    }\n    return A2, cache\n\ndef calculate_cost(A2, Y):\n    cost = -np.sum(np.multiply(Y, np.log(A2)) +  np.multiply(1-Y, np.log(1-A2)))/m\n    cost = np.squeeze(cost)\n\n    return cost\n\ndef backward_prop(X, Y, cache, parameters):\n    A1 = cache[\"A1\"]\n    A2 = cache[\"A2\"]\n\n    W2 = parameters[\"W2\"]\n\n    dZ2 = A2 - Y\n    dW2 = np.dot(dZ2, A1.T)/m\n    db2 = np.sum(dZ2, axis=1, keepdims=True)/m\n    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1-np.power(A1, 2))\n    dW1 = np.dot(dZ1, X.T)/m\n    db1 = np.sum(dZ1, axis=1, keepdims=True)/m\n\n    grads = {\n        \"dW1\": dW1,\n        \"db1\": db1,\n        \"dW2\": dW2,\n        \"db2\": db2\n    }\n\n    return grads\n\ndef update_parameters(parameters, grads, learning_rate):\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n\n    dW1 = grads[\"dW1\"]\n    db1 = grads[\"db1\"]\n    dW2 = grads[\"dW2\"]\n    db2 = grads[\"db2\"]\n\n    W1 = W1 - learning_rate*dW1\n    b1 = b1 - learning_rate*db1\n    W2 = W2 - learning_rate*dW2\n    b2 = b2 - learning_rate*db2\n    \n    new_parameters = {\n        \"W1\": W1,\n        \"W2\": W2,\n        \"b1\" : b1,\n        \"b2\" : b2\n    }\n\n    return new_parameters\n\n\ndef model(X, Y, n_x, n_h, n_y, num_of_iters, learning_rate):\n    parameters = initialize_parameters(n_x, n_h, n_y)\n\n    for i in range(0, num_of_iters+1):\n        a2, cache = forward_prop(X, parameters)\n\n        cost = calculate_cost(a2, Y)\n\n        grads = backward_prop(X, Y, cache, parameters)\n\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n        if(i%100 == 0):\n            print('Cost after iteration# {:d}: {:f}'.format(i, cost))\n\n    return parameters\n\ndef predict(X, parameters):\n    a2, cache = forward_prop(X, parameters)\n    yhat = a2\n    yhat = np.squeeze(yhat)\n    if(yhat >= 0.5):\n        y_predict = 1\n    else:\n        y_predict = 0\n\n    return y_predict\n    \n\n\nnp.random.seed(2)\n\n# The 4 training examples by columns\nX = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n\n# The outputs of the XOR for every example in X\nY = np.array([[1, 0, 0, 1]])\n\n# No. of training examples\nm = X.shape[1]\n\n# Set the hyperparameters\nn_x = 2     #No. of neurons in first layer\nn_h = 2     #No. of neurons in hidden layer\nn_y = 1     #No. of neurons in output layer\nnum_of_iters = 1000\nlearning_rate = 0.3\n\ntrained_parameters = model(X, Y, n_x, n_h, n_y, num_of_iters, learning_rate)\n\n# Test 2X1 vector to calculate the XOR of its elements. \n# Try (0, 0), (0, 1), (1, 0), (1, 1)\nX_test = np.array([[1], [1]])\n\ny_predict = predict(X_test, trained_parameters)\n\nprint('Neural Network prediction for example ({:d}, {:d}) is {:d}'.format(\n    X_test[0][0], X_test[1][0], y_predict))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}